# Stockholm AI RL Group Implementations
> In Stockholm AI reinforcement learning(RL) group, we discuss RL algorithms and their applications. In order to understand the algorithms fully, we focus on the theoratical and practical details of each algorithm including classic ones and deep ones. This repo is for our internal communication purpose.

We implemented our algorithms using gym and keras and after which we tested the algorithms on the simple cartpole example. The repo is implemented for people to understand the algorithm rather then receiving a good performance on this specific problem. We try to write the code in a clear way.

## Usage example

```sh
python cartpole.py
```

## Release History

* 0.2.2
	* Implemented the Proximal Policy Optimization method
* 0.2.1
    * Implemented Monte Carlo method
* 0.2.0
    * Fixed SARSA initialization problem
* 0.1.1
    * Implemented SARSA
* 0.1.0
    * Implemented the tabular Q learning
* 0.0.1
    * Work in progress

## LICENCE

[MIT Licence](https://en.wikipedia.org/wiki/MIT_License)

## Meta

Alex Yuan Gao – [@alexyuangao](https://twitter.com/alexyuangao) – gaoyuankidult@gmail.com

Distributed under the MIT license. See ``LICENSE`` for more information.

[https://github.com/gaoyuankidult/stockholm-ai-rl](https://github.com/stockholm-ai-rl/)

[npm-image]: https://img.shields.io/npm/v/datadog-metrics.svg?style=flat-square
[npm-url]: https://npmjs.org/package/datadog-metrics
[npm-downloads]: https://img.shields.io/npm/dm/datadog-metrics.svg?style=flat-square
[travis-image]: https://img.shields.io/travis/dbader/node-datadog-metrics/master.svg?style=flat-square
[travis-url]: https://travis-ci.org/dbader/node-datadog-metrics
